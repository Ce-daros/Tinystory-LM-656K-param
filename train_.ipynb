{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入必要库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('story_tokenizer_2048')\n",
    "dataset_name_or_path = \"TinyStoriesV2_SpecialTokens\"\n",
    "\n",
    "train_split = load_dataset(dataset_name_or_path, split='train').shuffle(seed=42).select(range(100000))\n",
    "\n",
    "print(train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(\n",
    "    examples: Dict[str, List]\n",
    ") -> Dict[str, List]:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('story_tokenizer_2048')\n",
    "    max_token = 512\n",
    "\n",
    "    encoded_texts = tokenizer(examples['text'], add_special_tokens=False)\n",
    "    input_ids_list = encoded_texts['input_ids']\n",
    "\n",
    "    new_input_ids_list, new_attn_mask_list = [], []\n",
    "    for input_ids in input_ids_list:\n",
    "        temp = input_ids[-max_token+1:] + [tokenizer.eos_token_id]\n",
    "        new_input_ids_list.append(temp)\n",
    "        new_attn_mask_list.append([1] * len(temp))\n",
    "    return {\n",
    "        \"input_ids\": new_input_ids_list,\n",
    "        \"attention_mask\": new_attn_mask_list\n",
    "    }\n",
    "\n",
    "num_proc = 8\n",
    "\n",
    "train_split = train_split.shuffle()\n",
    "\n",
    "train_split = train_split.map(\n",
    "    process_func,\n",
    "    batched=True,\n",
    "    num_proc=num_proc,\n",
    "    remove_columns=train_split.column_names,\n",
    "    desc='Running tokenizer on train_set: '\n",
    ")\n",
    "\n",
    "print(train_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "# 中间层取 8/3 倍，按 128 向上取整\n",
    "intermediate_size = (int(hidden_size * 8/3 / 128) + 1) * 128\n",
    "\n",
    "config = AutoConfig.for_model(\n",
    "    model_type=\"llama\",\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=2,\n",
    "    num_key_value_heads=4,\n",
    "    tie_word_embeddings=True,vocab_size=2048,max_position_embeddings=512\n",
    ")\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def init_model():\n",
    "    model = AutoModelForCausalLM.from_config(                    \n",
    "        config,\n",
    "        torch_dtype=torch.float32   # 全精度训练\n",
    "    ).to(device)                    # 迁移到 device 上\n",
    "\n",
    "    # Kaiming 初始化\n",
    "    def kaiming_initialization(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                torch.nn.init.kaiming_uniform_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
    "            elif 'bias' in name:\n",
    "                # 一般偏置项可以初始化为 0\n",
    "                torch.nn.init.constant_(param, 0)\n",
    "\n",
    "    kaiming_initialization(model)\n",
    "\n",
    "    def print_model_parameters(model):\n",
    "        print(\"Layer Name & Parameters\")\n",
    "        print(\"----------------------------\")\n",
    "        total_params = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            param_size = parameter.size()\n",
    "            param_count = torch.prod(torch.tensor(param_size)).item()\n",
    "            total_params += param_count\n",
    "            print(f\"{name:50} | Size: {str(param_size):30} | Count: {str(param_count):20}\")\n",
    "        print(\"----------------------------\")\n",
    "        print(f\"Total Parameters: {total_params} ({total_params / 1000000:.1f} M)\")\n",
    "\n",
    "    print_model_parameters(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_function(config=None):\n",
    "#     with wandb.init(config=config,project='huggingface'):\n",
    "#         # 从 wandb 获取超参数\n",
    "#         config = wandb.config\n",
    "\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#                 output_dir=\"saves\",\n",
    "#                 do_train=True,                              # 是否做训练\n",
    "#                 do_eval=False,                               # 是否做评估\n",
    "#                 per_device_train_batch_size=config.batch_size,              # 每设备批次\n",
    "#                 gradient_accumulation_steps=1,              # 梯度累计步大小，省显存，但小模型没必要，用 1 收敛比较快\n",
    "#                 learning_rate=config.learning_rate,                         # 学习率大小\n",
    "#                 lr_scheduler_type=config.lr_scheduler_type,                 # 学习率调度策略，LLM 训练一般都用余弦\n",
    "#                 bf16=True,\n",
    "#                 logging_steps=5,                           # 打印步骤间隔\n",
    "#                 num_train_epochs=2,                         # 训练轮数，2 ~ 3 即可\n",
    "#                 save_steps=10000000,                            # 检查点保存步骤间隔\n",
    "#                 seed=3407,report_to=None\n",
    "#             )\n",
    "\n",
    "#         trainer=Trainer(\n",
    "#             model=init_model(),\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_split,\n",
    "#             tokenizer=tokenizer,\n",
    "#             data_collator=data_collator,\n",
    "#         )\n",
    "#         trainer.train()\n",
    "\n",
    "# def scan():\n",
    "#     import os\n",
    "#     import wandb\n",
    "#     wandb.init( project='huggingface')\n",
    "#     sweep_config = {\n",
    "#         \"method\": \"bayes\",  # 使用贝叶斯优化\n",
    "#         \"metric\": {\n",
    "#             \"name\": \"train/loss\",  # 优化的指标是训练 loss\n",
    "#             \"goal\": \"minimize\",  # 目标是最小化训练 loss\n",
    "#         },\n",
    "#         \"parameters\": {\n",
    "#             \"learning_rate\": {\n",
    "#                 \"min\": 1e-4,\n",
    "#                 \"max\": 5e-3,\n",
    "#                 \"distribution\": \"log_uniform_values\",  # 学习率的分布是对数均匀分布\n",
    "#             },\n",
    "#             \"batch_size\": {\"values\": [16, 32, 64]},\"lr_scheduler_type\":{\"values\": [\"cosine\", \"constant\"]}\n",
    "#         },\n",
    "#     }\n",
    "#     os.environ[\"WANDB_TIMEOUT\"] = \"60\"\n",
    "#     sweep_id = wandb.sweep(sweep_config)\n",
    "#     wandb.agent(sweep_id, train_function)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                output_dir=\"saves\",\n",
    "                do_train=True,                              # 是否做训练\n",
    "                do_eval=False,                               # 是否做评估\n",
    "                per_device_train_batch_size=16,              # 每设备批次\n",
    "                gradient_accumulation_steps=1,              # 梯度累计步大小，省显存，但小模型没必要，用 1 收敛比较快\n",
    "                learning_rate=0.004629403549377777,                         # 学习率大小\n",
    "                lr_scheduler_type=\"constant\",                 # 学习率调度策略，LLM 训练一般都用余弦\n",
    "                bf16=True,\n",
    "                logging_steps=5,                           # 打印步骤间隔\n",
    "                num_train_epochs=2,                         # 训练轮数，2 ~ 3 即可\n",
    "                save_steps=10000000,                            # 检查点保存步骤间隔\n",
    "                seed=3407,report_to=None\n",
    "            )\n",
    "\n",
    "model=init_model()\n",
    "\n",
    "trainer=Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_split,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    input_text: str = \"Once upon a time, \",\n",
    "    max_new_tokens: int = 16\n",
    "):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.9,\n",
    "        temperature=0.6\n",
    "    )\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    # print(outputs)\n",
    "    print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little boy named Tim. Tim had a toy car that he loved to play with. One day, he went to the park with his mom. Tim saw a toy car on the ground. Tim wanted to play with the car to his mom and said, \"Mom, can I play with your car with my car too?\"\n",
      "His mom said, \"Yes, but we must not take turns.\" Tim felt sad, but he knew he had to go. He asked his mom for help. His mom said, \"Okay, let's clean it together.\" They went to play together and played the toy car. They had a lot of fun.\n",
      "After they finished the car together, Tim and his mom were surprised. They did not know that the car was not a toy car like it was a magic car. Tim had an idea. He put the car in the car and put the car on it. He pushed the car on the car on the car car and pulled it down. Tim was so happy. He played with the car with his car all day long, and Tim was very happy.<|end_story|>\n"
     ]
    }
   ],
   "source": [
    "inference(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    \"<|start_story|>Once upon a time, \",\n",
    "    max_new_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.bfloat16).save_pretrained(\"saves_bfloat16\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
